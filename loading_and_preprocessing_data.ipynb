{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94aeb6df-0060-489b-a3c1-61f7aea0adb0",
   "metadata": {},
   "source": [
    "**텐서플로에서 데이터 적재와 전처리하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c5066-1f34-4136-aad7-f6efad04aae3",
   "metadata": {},
   "source": [
    "# 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca0142c-37a4-4f45-878f-3d260f64d48b",
   "metadata": {},
   "source": [
    "먼저 몇 개의 모듈을 임포트한다. 맷플롯립 그림을 저장하는 함수를 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba63996f-8e17-44ce-9634-21064f8faafe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 공통 모듈 임포트\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = '.'\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, 'images')\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension='png', resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, f'{fig_id}.{fig_extension}')\n",
    "    print(f'그림 저장: {fig_id}')\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a04178-1c80-4b89-8d91-b26ee89f3ed8",
   "metadata": {},
   "source": [
    "## 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c44a50-3f27-4f2a-afde-8a1ae4d5deba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a20417-b118-475c-b02f-57534dc2e6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ada869-c2af-4fbc-b0f1-014960d2ddb5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d649651-76ab-42cb-975e-fe6abe245ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16da13c-a344-41e8-9e7f-481e4db2b284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcb7a241-1c52-4cad-a349-7276889da686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc9c686-e614-4423-9648-d319748993b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08486692-95c5-415c-9a84-4720ec3cee0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)  # < 10 항목만 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68bd97fb-6c86-4cac-97c2-da385b058bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d5e5906-ee02-4cf4-a5cb-3859d85759f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 1 4 0 3 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 5 0 2 1 3 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 5 7 6 8 0 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 1 4 6 7 8 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 2], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95173b9-e904-4164-815a-df316448b0df",
   "metadata": {},
   "source": [
    "## 캘리포니아 주택 데이터셋을 여러 개의 CSV로 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237a8fe-97e2-4358-87c5-eed6bdce8b39",
   "metadata": {},
   "source": [
    "캘리포니아 주택 데이터셋을 로드하고 준비한다. 먼저 로드한 다음 훈련 세트, 검증 세트, 테스트 세트로 나눈다. 마지막으로 스케일을 조정한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d70f8f-bd78-482d-ba33-bd3da0ab4331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1))\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77d837-f8e2-44f8-992c-bf2f08e89aba",
   "metadata": {},
   "source": [
    "메모리에 맞지 않는 매우 큰 데이터셋인 경우 일반적으로 먼저 여러 개의 파일로 나누고 텐서플로에서 이 파일들을 병렬로 읽게한다. 데모를 위해 주택 데이터셋을 20개의 CSV 파일로 나누어 본다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf139d49-4e20-4d88-bd9a-2766da8f80ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join('datasets', 'housing')\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, 'my_{}_{:02d}.csv')\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, 'wt', encoding='utf-8') as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write('\\n')\n",
    "            for row_idx in row_indices:\n",
    "                f.write(','.join([repr(col) for col in data[row_idx]]))\n",
    "                f.write('\\n')\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bded62a7-4f6c-478e-b1ab-7dd21b969a3c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + ['MedianHouseValue']\n",
    "header = ','.join(header_cols)\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, 'train', header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, 'valid', header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, 'test', header, n_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced7496-35e6-414c-a360-223abe17e913",
   "metadata": {},
   "source": [
    "좋다. 이 CSV 파일 중 하나에서 몇 줄을 출력해 본다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49680d3a-4fda-4659-922f-4d118063b398",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.8750</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.808344</td>\n",
       "      <td>1.065189</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>2.567145</td>\n",
       "      <td>32.82</td>\n",
       "      <td>-116.77</td>\n",
       "      <td>2.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.3333</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.668498</td>\n",
       "      <td>1.082418</td>\n",
       "      <td>1156.0</td>\n",
       "      <td>2.117216</td>\n",
       "      <td>34.16</td>\n",
       "      <td>-118.24</td>\n",
       "      <td>3.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.7540</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.642541</td>\n",
       "      <td>1.002954</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>3.397341</td>\n",
       "      <td>34.20</td>\n",
       "      <td>-118.59</td>\n",
       "      <td>1.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.7066</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>129.0</td>\n",
       "      <td>6.450000</td>\n",
       "      <td>34.07</td>\n",
       "      <td>-117.92</td>\n",
       "      <td>1.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.3185</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.210024</td>\n",
       "      <td>0.997613</td>\n",
       "      <td>1137.0</td>\n",
       "      <td>2.713604</td>\n",
       "      <td>34.18</td>\n",
       "      <td>-118.28</td>\n",
       "      <td>3.580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  2.8750      16.0  4.808344   1.065189      1969.0  2.567145     32.82   \n",
       "1  3.3333      40.0  4.668498   1.082418      1156.0  2.117216     34.16   \n",
       "2  2.7540      21.0  2.642541   1.002954      2300.0  3.397341     34.20   \n",
       "3  9.7066      38.0  8.750000   1.100000       129.0  6.450000     34.07   \n",
       "4  5.3185      52.0  6.210024   0.997613      1137.0  2.713604     34.18   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -116.77             2.229  \n",
       "1    -118.24             3.743  \n",
       "2    -118.59             1.798  \n",
       "3    -117.92             1.825  \n",
       "4    -118.28             3.580  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf1d9d-4bac-419d-ac36-29b9485ca026",
   "metadata": {},
   "source": [
    "텍스트 파일로 읽으면 다음과 같다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e93a14e5-8e6c-47de-ab6b-13b9ca742dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "2.875,16.0,4.808344198174707,1.0651890482398958,1969.0,2.5671447196870925,32.82,-116.77,2.229\n",
      "3.3333,40.0,4.668498168498169,1.0824175824175823,1156.0,2.1172161172161172,34.16,-118.24,3.743\n",
      "2.754,21.0,2.642540620384047,1.0029542097488922,2300.0,3.397341211225997,34.2,-118.59,1.798\n",
      "9.7066,38.0,8.75,1.1,129.0,6.45,34.07,-117.92,1.825\n"
     ]
    }
   ],
   "source": [
    "with open(train_filepaths[0]) as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline(), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5afaad16-8d3b-417f-b48a-040ea3d36b2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets\\\\housing\\\\my_train_00.csv',\n",
       " 'datasets\\\\housing\\\\my_train_01.csv',\n",
       " 'datasets\\\\housing\\\\my_train_02.csv',\n",
       " 'datasets\\\\housing\\\\my_train_03.csv',\n",
       " 'datasets\\\\housing\\\\my_train_04.csv',\n",
       " 'datasets\\\\housing\\\\my_train_05.csv',\n",
       " 'datasets\\\\housing\\\\my_train_06.csv',\n",
       " 'datasets\\\\housing\\\\my_train_07.csv',\n",
       " 'datasets\\\\housing\\\\my_train_08.csv',\n",
       " 'datasets\\\\housing\\\\my_train_09.csv',\n",
       " 'datasets\\\\housing\\\\my_train_10.csv',\n",
       " 'datasets\\\\housing\\\\my_train_11.csv',\n",
       " 'datasets\\\\housing\\\\my_train_12.csv',\n",
       " 'datasets\\\\housing\\\\my_train_13.csv',\n",
       " 'datasets\\\\housing\\\\my_train_14.csv',\n",
       " 'datasets\\\\housing\\\\my_train_15.csv',\n",
       " 'datasets\\\\housing\\\\my_train_16.csv',\n",
       " 'datasets\\\\housing\\\\my_train_17.csv',\n",
       " 'datasets\\\\housing\\\\my_train_18.csv',\n",
       " 'datasets\\\\housing\\\\my_train_19.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c52c64-d953-45eb-88e5-10703937ce52",
   "metadata": {},
   "source": [
    "## 입력 파이프라인 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adcdd6a5-bf71-41ad-b5df-12b5e2bb2b3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d956633a-dc80-42f4-92a5-573557c16a04",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_19.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd245571-ce13-4113-a557-d89b817f9bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f682ccf-e926-4f81-a7ba-af9758fdee31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'2.4375,52.0,5.019607843137255,0.9859943977591037,1049.0,2.9383753501400562,36.76,-119.81,0.571'\n",
      "b'2.675,37.0,3.404833836858006,0.9879154078549849,492.0,1.486404833836858,33.77,-118.17,2.417'\n",
      "b'5.1314,15.0,29.074766355140188,6.08411214953271,276.0,2.5794392523364484,37.06,-119.32,1.792'\n",
      "b'2.2656,23.0,6.690789473684211,1.4342105263157894,387.0,2.5460526315789473,39.9,-120.74,0.882'\n",
      "b'2.7321,28.0,5.043269230769231,1.1298076923076923,705.0,3.389423076923077,36.85,-121.79,1.5'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec080f-75e1-404e-af5a-01870a1e1eec",
   "metadata": {},
   "source": [
    "네 번째 필드의 4는 문자열로 해석된다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c350c65-aeba-4136-a3de-7ebff2c2074c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.0>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'4'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_defaults = [0, np.nan, tf.constant(np.nan, dtype=tf.float64), 'Hello', tf.constant([])]\n",
    "parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30544c4d-64e4-43f8-87c5-ba49a330d08b",
   "metadata": {},
   "source": [
    "누락된 값은 제공된 기본값으로 대체된다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba973488-9d49-41d3-885c-d000e804cf3a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'Hello'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_fields = tf.io.decode_csv(',,,,5', record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f290e83-7733-4d1c-a6cd-6500e26cefed",
   "metadata": {},
   "source": [
    "다섯 번째 필드는 필수이다(기본값을 `tf.constant([])`로 지정했기 때문에). 따라서 값을 전달하지 않으면 예외가 발생한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd59bbc5-9820-4ee0-96a4-a58d94f4612c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{function_node __wrapped__DecodeCSV_OUT_TYPE_5_device_/job:localhost/replica:0/task:0/device:CPU:0}} Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77fb746-0e1b-4315-bb6f-61d47d155594",
   "metadata": {},
   "source": [
    "필드 개수는 `record_defaults`에 있는 필드 개수와 정확히 맞아야 한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "636fd5df-3800-4942-b633-e2ae9af7d91e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{function_node __wrapped__DecodeCSV_OUT_TYPE_5_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5b71d60-1d02-4d30-a33b-81a0f3af275a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 8  # X_train.shape[-1]\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f32ef1cc-150e-4958-a2f8-b193e4de78bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16774078,  1.1986382 , -0.0455903 , -0.52083874, -0.5073813 ,\n",
       "        -0.46517077,  0.86297166, -1.2996906 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d2f4b3b-d5f2-4ec7-b483-a286954f4eda",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def csv_reader_dataset(\n",
    "        filepaths,\n",
    "        repeat=1,\n",
    "        n_readers=5,\n",
    "        n_read_threads=None,\n",
    "        shuffle_buffer_size=10000,\n",
    "        n_parse_threads=5,\n",
    "        batch_size=32\n",
    "):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers,\n",
    "        num_parallel_calls=n_read_threads\n",
    "    )\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a8a993a-a295-4079-998a-e25896d2c2e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.Tensor(\n",
      "[[ 1.0548339  -0.6253646   0.50441206 -0.25903296  0.489802    0.12015844\n",
      "  -0.63585055  0.42335564]\n",
      " [ 0.2619429   1.833074   -0.14657022 -0.15796822 -0.35886467 -0.26189885\n",
      "   0.7877957  -1.1453148 ]\n",
      " [ 1.7785524  -0.7839736   0.8398351  -0.42026544  0.62771034  0.4990901\n",
      "  -0.89426756  0.80680656]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[2.21 ]\n",
      " [2.236]\n",
      " [3.018]], shape=(3, 1), dtype=float32)\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[ 0.7675666  -0.3874512   0.6496367  -0.27316982  0.15121937  0.03598217\n",
      "  -1.3171314   1.2052004 ]\n",
      " [-0.6668316  -1.4977138  -0.15116398 -0.20659451  0.39255896 -0.269876\n",
      "   0.3367385  -0.01487831]\n",
      " [-0.46496966  0.16768008 -0.06912699 -0.12527053 -0.5533508  -0.25151363\n",
      "   1.9389265  -1.0257982 ]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[2.125]\n",
      " [0.803]\n",
      " [0.967]], shape=(3, 1), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
    "for X_batch, y_batch in train_set.take(2):\n",
    "    print('X =', X_batch)\n",
    "    print('y =', y_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "147b3815-5e40-4f94-9e4c-b97422eb842f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66b6906a-8fe5-440c-855e-72dd88c4c072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.backend.clear_session()\n",
    "model = keras.models.Sequential(\n",
    "    [keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]), keras.layers.Dense(1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa781255-63d9-4502-8d68-f6bcc52d2ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3362e5cd-ad95-491a-8fd2-dc245eaf9640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "362/362 [==============================] - 3s 7ms/step - loss: 2.1184 - val_loss: 1.5635\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.8452 - val_loss: 0.9477\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.6319 - val_loss: 0.7448\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.6016 - val_loss: 0.6533\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5436 - val_loss: 0.6126\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5294 - val_loss: 0.6034\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5078 - val_loss: 0.5449\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5023 - val_loss: 0.5304\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.4836 - val_loss: 0.5206\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.4627 - val_loss: 0.5456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb3c3335b0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "851f5fa4-6907-4197-a780-311ce2804a9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9282187819480896"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "256da1a2-4d9d-43c1-a291-e38fb3cb7000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.0731418],\n",
       "       [2.053124 ],\n",
       "       [3.3935633],\n",
       "       ...,\n",
       "       [1.8454084],\n",
       "       [2.3096504],\n",
       "       [3.0473442]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_set = test_set.map(lambda X, y: X)  # 대신 test_set을 전달할 수 있다. Keras는 레이블을 무시한다.\n",
    "X_new = X_test\n",
    "model.predict(new_set, steps=len(X_new) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d363e082-ce37-4be1-9efb-2b3ab368dd2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 1810/1810"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = len(X_train) // batch_size\n",
    "total_steps = n_epochs * n_steps_per_epoch\n",
    "global_step = 0\n",
    "for X_batch, y_batch in train_set.take(total_steps):\n",
    "    global_step += 1\n",
    "    print(f'\\rGlobal step {global_step}/{total_steps}', end='')\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X_batch)\n",
    "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "        loss = tf.add_n([main_loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcdbff17-0380-49db-89f6-b4ae25324455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c924413-7334-4b39-a13e-5bef2ce3a061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size=32, n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
    "    train_set = csv_reader_dataset(\n",
    "        train_filepaths,\n",
    "        repeat=n_epochs,\n",
    "        n_readers=n_readers,\n",
    "        n_read_threads=n_read_threads,\n",
    "        shuffle_buffer_size=shuffle_buffer_size,\n",
    "        n_parse_threads=n_parse_threads,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "\n",
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f169ac2c-fd08-48be-8324-e72f6933d2cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 100 / 1810\n",
      "Global step 200 / 1810\n",
      "Global step 300 / 1810\n",
      "Global step 400 / 1810\n",
      "Global step 500 / 1810\n",
      "Global step 600 / 1810\n",
      "Global step 700 / 1810\n",
      "Global step 800 / 1810\n",
      "Global step 900 / 1810\n",
      "Global step 1000 / 1810\n",
      "Global step 1100 / 1810\n",
      "Global step 1200 / 1810\n",
      "Global step 1300 / 1810\n",
      "Global step 1400 / 1810\n",
      "Global step 1500 / 1810\n",
      "Global step 1600 / 1810\n",
      "Global step 1700 / 1810\n",
      "Global step 1800 / 1810\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size=32, n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
    "    train_set = csv_reader_dataset(\n",
    "        train_filepaths,\n",
    "        repeat=n_epochs,\n",
    "        n_readers=n_readers,\n",
    "        n_read_threads=n_read_threads,\n",
    "        shuffle_buffer_size=shuffle_buffer_size,\n",
    "        n_parse_threads=n_parse_threads,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    n_steps_per_epoch = len(X_train) // batch_size\n",
    "    total_steps = n_epochs * n_steps_per_epoch\n",
    "    global_step = 0\n",
    "    for X_batch, y_batch in train_set.take(total_steps):\n",
    "        global_step += 1\n",
    "        if tf.equal(global_step % 100, 0):\n",
    "            tf.print(\"\\rGlobal step\", global_step, \"/\", total_steps)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "\n",
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7032e66-48af-498b-bef1-0765d2821a0b",
   "metadata": {},
   "source": [
    "`Dataset` 클래스에 있는 메서드의 간략한 설명이다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55653677-e463-4d0d-9583-7c96e6b13c87",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● apply()              Applies a transformation function to this dataset.\n",
      "● as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.\n",
      "● batch()              Combines consecutive elements of this dataset into batches.\n",
      "● bucket_by_sequence_length()A transformation that buckets elements in a `Dataset` by length.\n",
      "● cache()              Caches the elements in this dataset.\n",
      "● cardinality()        Returns the cardinality of the dataset, if known.\n",
      "● choose_from_datasets()Creates a dataset that deterministically chooses elements from `datasets`.\n",
      "● concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
      "● element_spec()       The type specification of an element of this dataset.\n",
      "● enumerate()          Enumerates the elements of this dataset.\n",
      "● filter()             Filters this dataset according to `predicate`.\n",
      "● flat_map()           Maps `map_func` across this dataset and flattens the result.\n",
      "● from_generator()     Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
      "● from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.\n",
      "● from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.\n",
      "● get_single_element() Returns the single element of the `dataset`.\n",
      "● group_by_window()    Groups windows of elements by key and reduces them.\n",
      "● interleave()         Maps `map_func` across this dataset, and interleaves the results.\n",
      "● list_files()         A dataset of all files matching one or more glob patterns.\n",
      "● load()               Loads a previously saved dataset.\n",
      "● map()                Maps `map_func` across the elements of this dataset.\n",
      "● options()            Returns the options for this dataset and its inputs.\n",
      "● padded_batch()       Combines consecutive elements of this dataset into padded batches.\n",
      "● prefetch()           Creates a `Dataset` that prefetches elements from this dataset.\n",
      "● random()             Creates a `Dataset` of pseudorandom values.\n",
      "● range()              Creates a `Dataset` of a step-separated range of values.\n",
      "● reduce()             Reduces the input dataset to a single element.\n",
      "● rejection_resample() A transformation that resamples a dataset to a target distribution.\n",
      "● repeat()             Repeats this dataset so each original value is seen `count` times.\n",
      "● sample_from_datasets()Samples elements at random from the datasets in `datasets`.\n",
      "● save()               Saves the content of the given dataset.\n",
      "● scan()               A transformation that scans a function across an input dataset.\n",
      "● shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      "● shuffle()            Randomly shuffles the elements of this dataset.\n",
      "● skip()               Creates a `Dataset` that skips `count` elements from this dataset.\n",
      "● snapshot()           API to persist the output of the input dataset.\n",
      "● take()               Creates a `Dataset` with at most `count` elements from this dataset.\n",
      "● take_while()         A transformation that stops dataset iteration based on a `predicate`.\n",
      "● unbatch()            Splits elements of a dataset into multiple elements.\n",
      "● unique()             A transformation that discards duplicate elements of a `Dataset`.\n",
      "● window()             Returns a dataset of \"windows\".\n",
      "● with_options()       Returns a new `tf.data.Dataset` with the given options set.\n",
      "● zip()                Creates a `Dataset` by zipping together the given datasets.\n"
     ]
    }
   ],
   "source": [
    "for m in dir(tf.data.Dataset):\n",
    "    if not (m.startswith('_') or m.endswith('_')):\n",
    "        func = getattr(tf.data.Dataset, m)\n",
    "        if hasattr(func, '__doc__'):\n",
    "            print('● {:21s}{}'.format(m + '()', func.__doc__.split('\\n')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996bd1e5-7ada-4264-83cf-116d8b350e85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `TFRecord` 이진 포맷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad136d1-3502-4de4-8faf-8bbd0f9a45eb",
   "metadata": {},
   "source": [
    "TFRecord 파일은 단순히 이진 레코드의 리스트이다. `tf.io.TFRecordWriter`를 사용해 만들 수 있다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15e59fbd-1d5e-4bea-81d8-ea7a71668182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter('my_data.tfrecord') as f:\n",
    "    f.write(b'This is the first record')\n",
    "    f.write(b'And this is the second record')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088bf166-076e-4104-940b-89317a55a98d",
   "metadata": {},
   "source": [
    "그리고 `tf.data.TFRecordDataset` 사용해 읽을 수 있다.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3243a48-e911-4976-a070-19f7f8b39c63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = ['my_data.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83928710-03ea-4de8-8b2e-a796c67a70f2",
   "metadata": {},
   "source": [
    "하나의 `TFRecordDataset`로 여러 개의 TFRecord 파일을 읽을 수 있다. 기본적으로 한 번에 하나의 파일만 읽지만 `num_parallel_reads=3`와 같이 지정하면 동시에 3개를 읽고 레코드를 번갈아 반환한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bec4d43-b7d9-4d53-82c2-b9495b973d23",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'File 0 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 1 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 2 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 0 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 1 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 2 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 0 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 1 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 2 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 3 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 4 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 3 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 4 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 3 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 4 record 2', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [f'my_test_{i}.tfrecord' for i in range(5)]\n",
    "for i, filepath in enumerate(filepaths):\n",
    "    with tf.io.TFRecordWriter(filepath) as f:\n",
    "        for j in range(3):\n",
    "            f.write(f'File {i} record {j}'.encode('utf-8'))\n",
    "dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=3)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00fd52dd-f97c-48c1-afb4-db06f8e3dc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter('my_compressed.tfrecord', options) as f:\n",
    "    f.write(b'This is the first record')\n",
    "    f.write(b'And this is the second record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69c8affe-273c-4176-99e4-0177507fbc5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset(['my_compressed.tfrecord'], compression_type='GZIP')\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c032b062-b9f7-4777-9fea-0919b1d03f0f",
   "metadata": {},
   "source": [
    "### 프로토콜 버퍼 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7bb553-aedf-475d-975e-733dd053d93d",
   "metadata": {},
   "source": [
    "이 절을 위해서는 [프로토콜 버퍼를 설치](https://developers.google.com/protocol-buffers/docs/downloads)해야 한다. 일반적으로 텐서플로를 사용할 때 프로토콜 버퍼를 설치할 필요는 없다. 텐서플로는 `tf.train.Example` 타입의 프로토콜 버퍼를 만들고 파싱할 수 있는 함수를 제공하며 보통의 경우 충분하다. 하지만 이 절에서는 자체적인 프로토콜 버퍼를 간단히 만들어 보겠다. 따라서 프로토콜 버퍼 컴파일러(`protoc`)가 필요하다. 이를 사용해 프로토콜 버퍼 정의를 컴파일하여 코드에서 사용할 수 있는 파이썬 모듈을 만들겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f3c68-d1c4-45b4-918a-06cae5d9a3a5",
   "metadata": {},
   "source": [
    "먼저 간단한 프로토콜 버퍼 정의를 작성해 본다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d1ee08b-4e82-4465-94fb-00da6583faaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting person.proto\n"
     ]
    }
   ],
   "source": [
    "%%writefile person.proto\n",
    "syntax = \"proto3\";\n",
    "message Person {\n",
    "  string name = 1;\n",
    "  int32 id = 2;\n",
    "  repeated string email = 3;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95f039-6c40-408a-96bb-12e2fea6090a",
   "metadata": {},
   "source": [
    "이 정의를 컴파일한다(`--descriptor_set_out`와 `--include_imports` 옵션은 아래 `tf.io.decode_proto()` 예제를 위해서 필요하다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "624f18c6-7ac8-4969-a41c-5039293d1e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef28a561-319d-44ed-ab47-4f13d54a9ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: C460-F1D1\n",
      "\n",
      " C:\\Users\\kyun\\Desktop\\loading_and_preprocessing_data 디렉터리\n",
      "\n",
      "2022-11-05  오후 09:40                92 person.desc\n",
      "2022-11-05  오후 09:40               108 person.proto\n",
      "2022-11-05  오후 09:40               986 person_pb2.py\n",
      "               3개 파일               1,186 바이트\n",
      "               0개 디렉터리  113,741,611,008 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "%ls person*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de557a26-0954-447e-ba8c-8a06daaf1b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"Al\"\n",
      "id: 123\n",
      "email: \"a@b.com\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from person_pb2 import Person\n",
    "\n",
    "person = Person(name='Al', id=123, email=['a@b.com'])  # Person 생성\n",
    "print(person)  # Person 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd5a88ff-e398-4904-80e4-fa10c6bb52e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Al'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.name  # 필드 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3a1f054-bcaa-40c9-add1-28ecb9935e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.name = 'Alice'  # 필드 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e34b5e5b-558e-47b2-9bcf-2b4d34fcf40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a@b.com'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.email[0]  # 배열처럼 사용할 수 있는 반복 필드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ac451e47-4f76-4136-9171-d61fef97729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.email.append('c@d.com')  # 이메일 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fca04668-9615-4649-8b6e-f2262a8499bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = person.SerializeToString()  # 바이트 문자열로 직렬화\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "470822a4-a70f-4f34-b4e5-1f16551c44ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person2 = Person()  # 새로운 Person 생성\n",
    "person2.ParseFromString(s)  # 바이트 문자열 파싱 (27 바이트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d553d13e-f0db-43bf-b5c4-d51635a9a693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person == person2  # 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdce45-c0c3-4068-b800-25257e16c3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
