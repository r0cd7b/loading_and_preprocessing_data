{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94aeb6df-0060-489b-a3c1-61f7aea0adb0",
   "metadata": {},
   "source": [
    "**텐서플로에서 데이터 적재와 전처리하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c5066-1f34-4136-aad7-f6efad04aae3",
   "metadata": {},
   "source": [
    "# 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca0142c-37a4-4f45-878f-3d260f64d48b",
   "metadata": {},
   "source": [
    "먼저 몇 개의 모듈을 임포트한다. 맷플롯립 그림을 저장하는 함수를 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba63996f-8e17-44ce-9634-21064f8faafe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 공통 모듈 임포트\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = '.'\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, 'images')\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension='png', resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, f'{fig_id}.{fig_extension}')\n",
    "    print(f'그림 저장: {fig_id}')\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a04178-1c80-4b89-8d91-b26ee89f3ed8",
   "metadata": {},
   "source": [
    "## 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c44a50-3f27-4f2a-afde-8a1ae4d5deba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a20417-b118-475c-b02f-57534dc2e6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ada869-c2af-4fbc-b0f1-014960d2ddb5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d649651-76ab-42cb-975e-fe6abe245ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16da13c-a344-41e8-9e7f-481e4db2b284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcb7a241-1c52-4cad-a349-7276889da686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc9c686-e614-4423-9648-d319748993b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08486692-95c5-415c-9a84-4720ec3cee0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)  # < 10 항목만 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68bd97fb-6c86-4cac-97c2-da385b058bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d5e5906-ee02-4cf4-a5cb-3859d85759f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 3 4 5 1 2 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 6 0 7 1 4 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([6 7 5 9 3 0 1], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 8 2 5 7 4 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95173b9-e904-4164-815a-df316448b0df",
   "metadata": {},
   "source": [
    "## 캘리포니아 주택 데이터셋을 여러 개의 CSV로 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237a8fe-97e2-4358-87c5-eed6bdce8b39",
   "metadata": {},
   "source": [
    "캘리포니아 주택 데이터셋을 로드하고 준비한다. 먼저 로드한 다음 훈련 세트, 검증 세트, 테스트 세트로 나눈다. 마지막으로 스케일을 조정한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d70f8f-bd78-482d-ba33-bd3da0ab4331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1))\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77d837-f8e2-44f8-992c-bf2f08e89aba",
   "metadata": {},
   "source": [
    "메모리에 맞지 않는 매우 큰 데이터셋인 경우 일반적으로 먼저 여러 개의 파일로 나누고 텐서플로에서 이 파일들을 병렬로 읽게한다. 데모를 위해 주택 데이터셋을 20개의 CSV 파일로 나누어 본다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf139d49-4e20-4d88-bd9a-2766da8f80ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join('datasets', 'housing')\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, 'my_{}_{:02d}.csv')\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, 'wt', encoding='utf-8') as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write('\\n')\n",
    "            for row_idx in row_indices:\n",
    "                f.write(','.join([repr(col) for col in data[row_idx]]))\n",
    "                f.write('\\n')\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bded62a7-4f6c-478e-b1ab-7dd21b969a3c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + ['MedianHouseValue']\n",
    "header = ','.join(header_cols)\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, 'train', header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, 'valid', header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, 'test', header, n_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced7496-35e6-414c-a360-223abe17e913",
   "metadata": {},
   "source": [
    "좋다. 이 CSV 파일 중 하나에서 몇 줄을 출력해 본다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49680d3a-4fda-4659-922f-4d118063b398",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5781</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.641509</td>\n",
       "      <td>1.287736</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2.452830</td>\n",
       "      <td>38.34</td>\n",
       "      <td>-122.40</td>\n",
       "      <td>2.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.8309</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.477876</td>\n",
       "      <td>0.933628</td>\n",
       "      <td>603.0</td>\n",
       "      <td>2.668142</td>\n",
       "      <td>34.18</td>\n",
       "      <td>-118.13</td>\n",
       "      <td>3.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.3768</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.906000</td>\n",
       "      <td>1.074000</td>\n",
       "      <td>1503.0</td>\n",
       "      <td>3.006000</td>\n",
       "      <td>34.07</td>\n",
       "      <td>-117.43</td>\n",
       "      <td>0.953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.2488</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.461538</td>\n",
       "      <td>0.903134</td>\n",
       "      <td>1324.0</td>\n",
       "      <td>3.772080</td>\n",
       "      <td>33.71</td>\n",
       "      <td>-117.90</td>\n",
       "      <td>2.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.6397</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.464286</td>\n",
       "      <td>1.050420</td>\n",
       "      <td>1672.0</td>\n",
       "      <td>3.512605</td>\n",
       "      <td>33.97</td>\n",
       "      <td>-118.11</td>\n",
       "      <td>1.666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5781      33.0  6.641509   1.287736       520.0  2.452830     38.34   \n",
       "1  5.8309      52.0  6.477876   0.933628       603.0  2.668142     34.18   \n",
       "2  2.3768      18.0  4.906000   1.074000      1503.0  3.006000     34.07   \n",
       "3  6.2488      16.0  5.461538   0.903134      1324.0  3.772080     33.71   \n",
       "4  3.6397      33.0  4.464286   1.050420      1672.0  3.512605     33.97   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.40             2.425  \n",
       "1    -118.13             3.091  \n",
       "2    -117.43             0.953  \n",
       "3    -117.90             2.520  \n",
       "4    -118.11             1.666  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf1d9d-4bac-419d-ac36-29b9485ca026",
   "metadata": {},
   "source": [
    "텍스트 파일로 읽으면 다음과 같다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e93a14e5-8e6c-47de-ab6b-13b9ca742dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5781,33.0,6.6415094339622645,1.2877358490566038,520.0,2.452830188679245,38.34,-122.4,2.425\n",
      "5.8309,52.0,6.477876106194691,0.9336283185840708,603.0,2.668141592920354,34.18,-118.13,3.091\n",
      "2.3768,18.0,4.906,1.074,1503.0,3.006,34.07,-117.43,0.953\n",
      "6.2488,16.0,5.461538461538462,0.9031339031339032,1324.0,3.7720797720797723,33.71,-117.9,2.52\n"
     ]
    }
   ],
   "source": [
    "with open(train_filepaths[0]) as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline(), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5afaad16-8d3b-417f-b48a-040ea3d36b2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets\\\\housing\\\\my_train_00.csv',\n",
       " 'datasets\\\\housing\\\\my_train_01.csv',\n",
       " 'datasets\\\\housing\\\\my_train_02.csv',\n",
       " 'datasets\\\\housing\\\\my_train_03.csv',\n",
       " 'datasets\\\\housing\\\\my_train_04.csv',\n",
       " 'datasets\\\\housing\\\\my_train_05.csv',\n",
       " 'datasets\\\\housing\\\\my_train_06.csv',\n",
       " 'datasets\\\\housing\\\\my_train_07.csv',\n",
       " 'datasets\\\\housing\\\\my_train_08.csv',\n",
       " 'datasets\\\\housing\\\\my_train_09.csv',\n",
       " 'datasets\\\\housing\\\\my_train_10.csv',\n",
       " 'datasets\\\\housing\\\\my_train_11.csv',\n",
       " 'datasets\\\\housing\\\\my_train_12.csv',\n",
       " 'datasets\\\\housing\\\\my_train_13.csv',\n",
       " 'datasets\\\\housing\\\\my_train_14.csv',\n",
       " 'datasets\\\\housing\\\\my_train_15.csv',\n",
       " 'datasets\\\\housing\\\\my_train_16.csv',\n",
       " 'datasets\\\\housing\\\\my_train_17.csv',\n",
       " 'datasets\\\\housing\\\\my_train_18.csv',\n",
       " 'datasets\\\\housing\\\\my_train_19.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c52c64-d953-45eb-88e5-10703937ce52",
   "metadata": {},
   "source": [
    "## 입력 파이프라인 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adcdd6a5-bf71-41ad-b5df-12b5e2bb2b3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d956633a-dc80-42f4-92a5-573557c16a04",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_02.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd245571-ce13-4113-a557-d89b817f9bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f682ccf-e926-4f81-a7ba-af9758fdee31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4.1375,38.0,5.095070422535211,1.0316901408450705,1042.0,3.6690140845070425,34.03,-118.14,2.115'\n",
      "b'3.6389,42.0,3.994296577946768,0.9961977186311787,1212.0,2.3041825095057034,37.34,-121.92,2.392'\n",
      "b'2.8061,14.0,4.8798076923076925,1.0721153846153846,1974.0,1.5817307692307692,33.66,-117.97,1.803'\n",
      "b'7.8627,24.0,7.1451149425287355,1.0186781609195403,1692.0,2.4310344827586206,37.26,-122.04,5.00001'\n",
      "b'7.9213,44.0,6.44811320754717,0.9787735849056604,1057.0,2.4929245283018866,34.13,-118.18,4.778'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec080f-75e1-404e-af5a-01870a1e1eec",
   "metadata": {},
   "source": [
    "네 번째 필드의 4는 문자열로 해석된다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c350c65-aeba-4136-a3de-7ebff2c2074c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.0>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'4'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_defaults = [0, np.nan, tf.constant(np.nan, dtype=tf.float64), 'Hello', tf.constant([])]\n",
    "parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30544c4d-64e4-43f8-87c5-ba49a330d08b",
   "metadata": {},
   "source": [
    "누락된 값은 제공된 기본값으로 대체된다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba973488-9d49-41d3-885c-d000e804cf3a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'Hello'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_fields = tf.io.decode_csv(',,,,5', record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f290e83-7733-4d1c-a6cd-6500e26cefed",
   "metadata": {},
   "source": [
    "다섯 번째 필드는 필수이다(기본값을 `tf.constant([])`로 지정했기 때문에). 따라서 값을 전달하지 않으면 예외가 발생한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd59bbc5-9820-4ee0-96a4-a58d94f4612c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{function_node __wrapped__DecodeCSV_OUT_TYPE_5_device_/job:localhost/replica:0/task:0/device:CPU:0}} Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77fb746-0e1b-4315-bb6f-61d47d155594",
   "metadata": {},
   "source": [
    "필드 개수는 `record_defaults`에 있는 필드 개수와 정확히 맞아야 한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "636fd5df-3800-4942-b633-e2ae9af7d91e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{function_node __wrapped__DecodeCSV_OUT_TYPE_5_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5b71d60-1d02-4d30-a33b-81a0f3af275a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 8  # X_train.shape[-1]\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f32ef1cc-150e-4958-a2f8-b193e4de78bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.1713626 ,  1.2126731 , -0.04916688, -0.4360729 , -0.49995568,\n",
       "        -0.06143444,  0.85889053, -1.3097554 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d2f4b3b-d5f2-4ec7-b483-a286954f4eda",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def csv_reader_dataset(\n",
    "        filepaths,\n",
    "        repeat=1,\n",
    "        n_readers=5,\n",
    "        n_read_threads=None,\n",
    "        shuffle_buffer_size=10000,\n",
    "        n_parse_threads=5,\n",
    "        batch_size=32\n",
    "):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers,\n",
    "        num_parallel_calls=n_read_threads\n",
    "    )\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a8a993a-a295-4079-998a-e25896d2c2e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.Tensor(\n",
      "[[-0.92626244 -1.4098945   8.284036    9.780367   -0.92926496 -0.08102798\n",
      "   0.22794043  0.51490444]\n",
      " [-0.03680772 -1.7277814   0.18190964 -0.20809142  0.7485458  -0.01352079\n",
      "   1.6487447  -1.0206043 ]\n",
      " [ 0.44563833  0.6563708  -0.13266188 -0.33095726 -0.13110015  0.01380935\n",
      "   0.9009539  -1.3596127 ]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[0.991]\n",
      " [1.107]\n",
      " [2.718]], shape=(3, 1), dtype=float32)\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[-0.31191775 -0.93306404 -0.2022773  -0.25429323 -0.45702475 -0.07035509\n",
      "   1.307565   -1.5440706 ]\n",
      " [ 3.358875    0.2590121   1.0735925  -0.19028421  0.5978495  -0.0206938\n",
      "   1.0598596  -1.2798442 ]\n",
      " [ 0.39944795 -0.37676182  0.36133352 -0.09664767  0.13261841 -0.05216635\n",
      "  -0.8470095   0.75420606]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[2.027  ]\n",
      " [5.00001]\n",
      " [2.134  ]], shape=(3, 1), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
    "for X_batch, y_batch in train_set.take(2):\n",
    "    print('X =', X_batch)\n",
    "    print('y =', y_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "147b3815-5e40-4f94-9e4c-b97422eb842f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66b6906a-8fe5-440c-855e-72dd88c4c072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.backend.clear_session()\n",
    "model = keras.models.Sequential(\n",
    "    [keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]), keras.layers.Dense(1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa781255-63d9-4502-8d68-f6bcc52d2ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3362e5cd-ad95-491a-8fd2-dc245eaf9640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "362/362 [==============================] - 3s 7ms/step - loss: 2.2148 - val_loss: 0.9532\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.7905 - val_loss: 0.7098\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.6917 - val_loss: 0.6507\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.6353 - val_loss: 0.6091\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.6143 - val_loss: 0.5870\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5993 - val_loss: 0.5665\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5632 - val_loss: 0.5494\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5673 - val_loss: 0.5379\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5066 - val_loss: 0.5301\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5291 - val_loss: 0.5263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26e9781e0b0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "851f5fa4-6907-4197-a780-311ce2804a9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5311558842658997"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "256da1a2-4d9d-43c1-a291-e38fb3cb7000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.032022 ],\n",
       "       [1.4179717],\n",
       "       [2.871379 ],\n",
       "       ...,\n",
       "       [2.464898 ],\n",
       "       [2.1004734],\n",
       "       [2.102226 ]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_set = test_set.map(lambda X, y: X)  # 대신 test_set을 전달할 수 있다. Keras는 레이블을 무시한다.\n",
    "X_new = X_test\n",
    "model.predict(new_set, steps=len(X_new) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d363e082-ce37-4be1-9efb-2b3ab368dd2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 1810/1810"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = len(X_train) // batch_size\n",
    "total_steps = n_epochs * n_steps_per_epoch\n",
    "global_step = 0\n",
    "for X_batch, y_batch in train_set.take(total_steps):\n",
    "    global_step += 1\n",
    "    print(f'\\rGlobal step {global_step}/{total_steps}', end='')\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X_batch)\n",
    "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "        loss = tf.add_n([main_loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcdbff17-0380-49db-89f6-b4ae25324455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c924413-7334-4b39-a13e-5bef2ce3a061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size=32, n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
    "    train_set = csv_reader_dataset(\n",
    "        train_filepaths,\n",
    "        repeat=n_epochs,\n",
    "        n_readers=n_readers,\n",
    "        n_read_threads=n_read_threads,\n",
    "        shuffle_buffer_size=shuffle_buffer_size,\n",
    "        n_parse_threads=n_parse_threads,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "\n",
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f169ac2c-fd08-48be-8324-e72f6933d2cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 100 / 1810\n",
      "Global step 200 / 1810\n",
      "Global step 300 / 1810\n",
      "Global step 400 / 1810\n",
      "Global step 500 / 1810\n",
      "Global step 600 / 1810\n",
      "Global step 700 / 1810\n",
      "Global step 800 / 1810\n",
      "Global step 900 / 1810\n",
      "Global step 1000 / 1810\n",
      "Global step 1100 / 1810\n",
      "Global step 1200 / 1810\n",
      "Global step 1300 / 1810\n",
      "Global step 1400 / 1810\n",
      "Global step 1500 / 1810\n",
      "Global step 1600 / 1810\n",
      "Global step 1700 / 1810\n",
      "Global step 1800 / 1810\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size=32, n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
    "    train_set = csv_reader_dataset(\n",
    "        train_filepaths,\n",
    "        repeat=n_epochs,\n",
    "        n_readers=n_readers,\n",
    "        n_read_threads=n_read_threads,\n",
    "        shuffle_buffer_size=shuffle_buffer_size,\n",
    "        n_parse_threads=n_parse_threads,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    n_steps_per_epoch = len(X_train) // batch_size\n",
    "    total_steps = n_epochs * n_steps_per_epoch\n",
    "    global_step = 0\n",
    "    for X_batch, y_batch in train_set.take(total_steps):\n",
    "        global_step += 1\n",
    "        if tf.equal(global_step % 100, 0):\n",
    "            tf.print(\"\\rGlobal step\", global_step, \"/\", total_steps)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "\n",
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7032e66-48af-498b-bef1-0765d2821a0b",
   "metadata": {},
   "source": [
    "`Dataset` 클래스에 있는 메서드의 간략한 설명이다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55653677-e463-4d0d-9583-7c96e6b13c87",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● apply()              Applies a transformation function to this dataset.\n",
      "● as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.\n",
      "● batch()              Combines consecutive elements of this dataset into batches.\n",
      "● bucket_by_sequence_length()A transformation that buckets elements in a `Dataset` by length.\n",
      "● cache()              Caches the elements in this dataset.\n",
      "● cardinality()        Returns the cardinality of the dataset, if known.\n",
      "● choose_from_datasets()Creates a dataset that deterministically chooses elements from `datasets`.\n",
      "● concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
      "● element_spec()       The type specification of an element of this dataset.\n",
      "● enumerate()          Enumerates the elements of this dataset.\n",
      "● filter()             Filters this dataset according to `predicate`.\n",
      "● flat_map()           Maps `map_func` across this dataset and flattens the result.\n",
      "● from_generator()     Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
      "● from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.\n",
      "● from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.\n",
      "● get_single_element() Returns the single element of the `dataset`.\n",
      "● group_by_window()    Groups windows of elements by key and reduces them.\n",
      "● interleave()         Maps `map_func` across this dataset, and interleaves the results.\n",
      "● list_files()         A dataset of all files matching one or more glob patterns.\n",
      "● load()               Loads a previously saved dataset.\n",
      "● map()                Maps `map_func` across the elements of this dataset.\n",
      "● options()            Returns the options for this dataset and its inputs.\n",
      "● padded_batch()       Combines consecutive elements of this dataset into padded batches.\n",
      "● prefetch()           Creates a `Dataset` that prefetches elements from this dataset.\n",
      "● random()             Creates a `Dataset` of pseudorandom values.\n",
      "● range()              Creates a `Dataset` of a step-separated range of values.\n",
      "● reduce()             Reduces the input dataset to a single element.\n",
      "● rejection_resample() A transformation that resamples a dataset to a target distribution.\n",
      "● repeat()             Repeats this dataset so each original value is seen `count` times.\n",
      "● sample_from_datasets()Samples elements at random from the datasets in `datasets`.\n",
      "● save()               Saves the content of the given dataset.\n",
      "● scan()               A transformation that scans a function across an input dataset.\n",
      "● shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      "● shuffle()            Randomly shuffles the elements of this dataset.\n",
      "● skip()               Creates a `Dataset` that skips `count` elements from this dataset.\n",
      "● snapshot()           API to persist the output of the input dataset.\n",
      "● take()               Creates a `Dataset` with at most `count` elements from this dataset.\n",
      "● take_while()         A transformation that stops dataset iteration based on a `predicate`.\n",
      "● unbatch()            Splits elements of a dataset into multiple elements.\n",
      "● unique()             A transformation that discards duplicate elements of a `Dataset`.\n",
      "● window()             Returns a dataset of \"windows\".\n",
      "● with_options()       Returns a new `tf.data.Dataset` with the given options set.\n",
      "● zip()                Creates a `Dataset` by zipping together the given datasets.\n"
     ]
    }
   ],
   "source": [
    "for m in dir(tf.data.Dataset):\n",
    "    if not (m.startswith('_') or m.endswith('_')):\n",
    "        func = getattr(tf.data.Dataset, m)\n",
    "        if hasattr(func, '__doc__'):\n",
    "            print('● {:21s}{}'.format(m + '()', func.__doc__.split('\\n')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578aa374-d81a-4c01-b4dd-d6178654f8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
